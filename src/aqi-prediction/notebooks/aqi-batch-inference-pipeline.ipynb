{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d2kLrOh-bpGy"
   },
   "source": [
    "# Iris Flower - Batch Prediction\n",
    "\n",
    "\n",
    "In this notebook we will, \n",
    "\n",
    "1. Load the batch inference data that arrived in the last 24 hours\n",
    "2. Predict the first Iris Flower found in the batch\n",
    "3. Write the ouput png of the Iris flower predicted, to be displayed in Github Pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "xRtpj-psbpG8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-01-03 01:46:30,812 INFO: Initializing external client\n",
      "2025-01-03 01:46:30,813 INFO: Base URL: https://c.app.hopsworks.ai:443\n",
      "2025-01-03 01:46:34,142 INFO: Python Engine initialized.\n",
      "\n",
      "Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/1207459\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import hopsworks\n",
    "import joblib\n",
    "\n",
    "project = hopsworks.login()\n",
    "\n",
    "fs = project.get_feature_store()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading model artifact (0 dirs, 1 files)... DONE\r"
     ]
    }
   ],
   "source": [
    "mr = project.get_model_registry()\n",
    "model = mr.get_model(\"aqi\", version=1)\n",
    "model_dir = model.download()\n",
    "model = joblib.load(model_dir + \"/aqi.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are downloading the 'raw' iris data. We explicitly do not want transformed data, reading for training. \n",
    "\n",
    "So, let's download the iris dataset, and preview some rows. \n",
    "\n",
    "Note, that it is 'tabular data'. There are 5 columns: 4 of them are \"features\", and the \"variety\" column is the **target** (what we are trying to predict using the 4 feature values in the target's row)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "nRmFM7vcbpHA",
    "outputId": "d920d168-9818-40c5-c292-4cf0afcbbcfd"
   },
   "outputs": [],
   "source": [
    "feature_view = fs.get_feature_view(name=\"aqi\", version=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will do some **Batch Inference**. \n",
    "\n",
    "We will read all the input features that have arrived in the last 24 hours, and score them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "uHuAD3ttP8Ep"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished: Reading data from Hopsworks, using Hopsworks Feature Query Service (2.04s) \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['Good', 'Good', 'Good', ..., 'Good', 'Good', 'Hazardous'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime\n",
    "from PIL import Image\n",
    "\n",
    "#the batch data is the feature data, i.e without the labels\n",
    "batch_data = feature_view.get_batch_data()\n",
    "\n",
    "y_pred = model.predict(batch_data)\n",
    "\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         49\n",
       "1         37\n",
       "2         48\n",
       "3         61\n",
       "4         54\n",
       "        ... \n",
       "16389    114\n",
       "16390     41\n",
       "16391     43\n",
       "16392     34\n",
       "16393    303\n",
       "Name: aqi_value, Length: 16394, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_data['aqi_value']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch prediction output is the last entry in the batch - it is output as a file 'latest_iris.png'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#image of the predicted flower, the latest data added to the feature store\n",
    "category = y_pred[y_pred.size-1]\n",
    "# flower_img = \"assets/\" + flower + \".png\"\n",
    "# img = Image.open(flower_img)            \n",
    "\n",
    "# img.save(\"../../assets/latest_iris.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished: Reading data from Hopsworks, using Hopsworks Feature Query Service (4.87s) \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country</th>\n",
       "      <th>city</th>\n",
       "      <th>aqi_value</th>\n",
       "      <th>aqi_category</th>\n",
       "      <th>co_aqi_value</th>\n",
       "      <th>ozone_aqi_value</th>\n",
       "      <th>no2_aqi_value</th>\n",
       "      <th>pm25_aqi_value</th>\n",
       "      <th>lat</th>\n",
       "      <th>lng</th>\n",
       "      <th>uuid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Germany</td>\n",
       "      <td>Wolgast</td>\n",
       "      <td>49</td>\n",
       "      <td>Good</td>\n",
       "      <td>1</td>\n",
       "      <td>35</td>\n",
       "      <td>2</td>\n",
       "      <td>49</td>\n",
       "      <td>54.0500</td>\n",
       "      <td>13.7667</td>\n",
       "      <td>ad2eed88-7404-4fea-83e1-a5365da42f69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Belgium</td>\n",
       "      <td>Ans</td>\n",
       "      <td>37</td>\n",
       "      <td>Good</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>4</td>\n",
       "      <td>37</td>\n",
       "      <td>50.6625</td>\n",
       "      <td>5.5200</td>\n",
       "      <td>346747f4-55c0-469d-81a2-cae081a4a615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Belgium</td>\n",
       "      <td>Tubize</td>\n",
       "      <td>48</td>\n",
       "      <td>Good</td>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>5</td>\n",
       "      <td>48</td>\n",
       "      <td>50.6930</td>\n",
       "      <td>4.2047</td>\n",
       "      <td>a5ffad51-2116-41d0-bafa-65b2aeac110d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Romania</td>\n",
       "      <td>Gheorgheni</td>\n",
       "      <td>61</td>\n",
       "      <td>Moderate</td>\n",
       "      <td>1</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>61</td>\n",
       "      <td>46.7200</td>\n",
       "      <td>25.5900</td>\n",
       "      <td>3d62cb08-f644-478e-b18e-9d91780a7d62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>United States of America</td>\n",
       "      <td>State College</td>\n",
       "      <td>54</td>\n",
       "      <td>Moderate</td>\n",
       "      <td>1</td>\n",
       "      <td>40</td>\n",
       "      <td>1</td>\n",
       "      <td>54</td>\n",
       "      <td>40.7909</td>\n",
       "      <td>-77.8567</td>\n",
       "      <td>b03c92c8-5507-440f-a514-32e4ab235dd4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16389</th>\n",
       "      <td>India</td>\n",
       "      <td>Barauli</td>\n",
       "      <td>114</td>\n",
       "      <td>Unhealthy for Sensitive Groups</td>\n",
       "      <td>3</td>\n",
       "      <td>67</td>\n",
       "      <td>2</td>\n",
       "      <td>114</td>\n",
       "      <td>26.3815</td>\n",
       "      <td>84.5872</td>\n",
       "      <td>4b0be1d1-8a70-4e76-93ba-99b7932dd866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16390</th>\n",
       "      <td>United States of America</td>\n",
       "      <td>Albany</td>\n",
       "      <td>41</td>\n",
       "      <td>Good</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>8</td>\n",
       "      <td>41</td>\n",
       "      <td>44.6272</td>\n",
       "      <td>-123.0965</td>\n",
       "      <td>d6ad878d-33c5-462d-bde7-9b1eb7b6d703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16391</th>\n",
       "      <td>Switzerland</td>\n",
       "      <td>Winterthur</td>\n",
       "      <td>43</td>\n",
       "      <td>Good</td>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>2</td>\n",
       "      <td>43</td>\n",
       "      <td>47.4989</td>\n",
       "      <td>8.7286</td>\n",
       "      <td>3168e7be-9301-4b27-8b01-611b36ad0556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16392</th>\n",
       "      <td>Finland</td>\n",
       "      <td>Salo</td>\n",
       "      <td>34</td>\n",
       "      <td>Good</td>\n",
       "      <td>0</td>\n",
       "      <td>34</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>60.3861</td>\n",
       "      <td>23.1250</td>\n",
       "      <td>d8a12282-c99d-4197-8e2c-cfdddbff3ed9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16393</th>\n",
       "      <td>Italy</td>\n",
       "      <td>Massarosa</td>\n",
       "      <td>303</td>\n",
       "      <td>Hazardous</td>\n",
       "      <td>16</td>\n",
       "      <td>123</td>\n",
       "      <td>58</td>\n",
       "      <td>200</td>\n",
       "      <td>43.8667</td>\n",
       "      <td>10.3333</td>\n",
       "      <td>cee2bf5f-9542-4aff-bbea-13ee87634eab</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16394 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        country           city  aqi_value  \\\n",
       "0                       Germany        Wolgast         49   \n",
       "1                       Belgium            Ans         37   \n",
       "2                       Belgium         Tubize         48   \n",
       "3                       Romania     Gheorgheni         61   \n",
       "4      United States of America  State College         54   \n",
       "...                         ...            ...        ...   \n",
       "16389                     India        Barauli        114   \n",
       "16390  United States of America         Albany         41   \n",
       "16391               Switzerland     Winterthur         43   \n",
       "16392                   Finland           Salo         34   \n",
       "16393                     Italy      Massarosa        303   \n",
       "\n",
       "                         aqi_category  co_aqi_value  ozone_aqi_value  \\\n",
       "0                                Good             1               35   \n",
       "1                                Good             1               22   \n",
       "2                                Good             1               25   \n",
       "3                            Moderate             1               40   \n",
       "4                            Moderate             1               40   \n",
       "...                               ...           ...              ...   \n",
       "16389  Unhealthy for Sensitive Groups             3               67   \n",
       "16390                            Good             1               15   \n",
       "16391                            Good             1               25   \n",
       "16392                            Good             0               34   \n",
       "16393                       Hazardous            16              123   \n",
       "\n",
       "       no2_aqi_value  pm25_aqi_value      lat       lng  \\\n",
       "0                  2              49  54.0500   13.7667   \n",
       "1                  4              37  50.6625    5.5200   \n",
       "2                  5              48  50.6930    4.2047   \n",
       "3                  0              61  46.7200   25.5900   \n",
       "4                  1              54  40.7909  -77.8567   \n",
       "...              ...             ...      ...       ...   \n",
       "16389              2             114  26.3815   84.5872   \n",
       "16390              8              41  44.6272 -123.0965   \n",
       "16391              2              43  47.4989    8.7286   \n",
       "16392              1              20  60.3861   23.1250   \n",
       "16393             58             200  43.8667   10.3333   \n",
       "\n",
       "                                       uuid  \n",
       "0      ad2eed88-7404-4fea-83e1-a5365da42f69  \n",
       "1      346747f4-55c0-469d-81a2-cae081a4a615  \n",
       "2      a5ffad51-2116-41d0-bafa-65b2aeac110d  \n",
       "3      3d62cb08-f644-478e-b18e-9d91780a7d62  \n",
       "4      b03c92c8-5507-440f-a514-32e4ab235dd4  \n",
       "...                                     ...  \n",
       "16389  4b0be1d1-8a70-4e76-93ba-99b7932dd866  \n",
       "16390  d6ad878d-33c5-462d-bde7-9b1eb7b6d703  \n",
       "16391  3168e7be-9301-4b27-8b01-611b36ad0556  \n",
       "16392  d8a12282-c99d-4197-8e2c-cfdddbff3ed9  \n",
       "16393  cee2bf5f-9542-4aff-bbea-13ee87634eab  \n",
       "\n",
       "[16394 rows x 11 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aqi_fg = fs.get_feature_group(name=\"aqi\", version=1)\n",
    "df = aqi_fg.read()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hazardous'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label = df.iloc[-1][\"aqi_category\"]\n",
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#image of the actual flower\n",
    "# label_flower = \"assets/\" + label + \".png\"\n",
    "\n",
    "# img = Image.open(label_flower)            \n",
    "\n",
    "# img.save(\"../../assets/actual_iris.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "monitor_fg = fs.get_or_create_feature_group(name=\"aqi_predictions\",\n",
    "                                  version=1,\n",
    "                                  primary_key=[\"datetime\"],\n",
    "                                  description=\"Air Quality Prediction/Outcome Monitoring\"\n",
    "                                 )\n",
    "\n",
    "# Clear the contents of the feature group\n",
    "# monitor_fg = fs.get_feature_group(name=\"iris_predictions\", version=1)\n",
    "\n",
    "# monitor_fg.delete()\n",
    "\n",
    "# print(\"Feature group contents cleared successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Group created successfully, explore it at \n",
      "https://c.app.hopsworks.ai:443/p/1207459/fs/1195092/fg/1393475\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading Dataframe: 100.00% |██████████| Rows 1/1 | Elapsed Time: 00:00 | Remaining Time: 00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching job: aqi_predictions_1_offline_fg_materialization\n",
      "Job started successfully, you can follow the progress at \n",
      "https://c.app.hopsworks.ai:443/p/1207459/jobs/named/aqi_predictions_1_offline_fg_materialization/executions\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Job('aqi_predictions_1_offline_fg_materialization', 'SPARK'), None)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "now = datetime.now().strftime(\"%m/%d/%Y, %H:%M:%S\")\n",
    "last_row = df.iloc[-1]\n",
    "data = {\n",
    "    'country': [last_row['country']],  # Convert to list for DataFrame compatibility\n",
    "    'city': [last_row['city']],\n",
    "    'aqi_value': [last_row['aqi_value']],\n",
    "    'co_aqi_value': [last_row['co_aqi_value']],\n",
    "    'ozone_aqi_value': [last_row['ozone_aqi_value']],\n",
    "    'no2_aqi_value': [last_row['no2_aqi_value']],\n",
    "    'pm25_aqi_value': [last_row['pm25_aqi_value']],\n",
    "    'lat': [last_row['lat']],\n",
    "    'lng': [last_row['lng']],\n",
    "    'prediction': [category],  # Ensure 'flower' is a scalar value\n",
    "    'label': [label],        # Ensure 'label' is a scalar value\n",
    "    'datetime': [now],\n",
    "}\n",
    "monitor_df = pd.DataFrame(data)\n",
    "monitor_fg.insert(monitor_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-01-03 01:48:06,540 ERROR: [Errno 2] Opening HDFS file '/apps/hive/warehouse/mlops101_featurestore.db/aqi_predictions_1/.hoodie/hoodie.properties' failed. Detail: [errno 2] No such file or directory. Detail: Python exception: FlyingDuckException. gRPC client debug context: UNKNOWN:Error received from peer ipv4:51.79.26.27:5005 {grpc_message:\"[Errno 2] Opening HDFS file \\'/apps/hive/warehouse/mlops101_featurestore.db/aqi_predictions_1/.hoodie/hoodie.properties\\' failed. Detail: [errno 2] No such file or directory. Detail: Python exception: FlyingDuckException\", grpc_status:2, created_time:\"2025-01-03T00:48:06.541484656+00:00\"}. Client context: IOError: Server never sent a data message. Detail: Internal\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\HP\\anaconda3\\Lib\\site-packages\\hsfs\\core\\arrow_flight_client.py\", line 364, in afs_error_handler_wrapper\n",
      "    return func(instance, *args, **kw)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\HP\\anaconda3\\Lib\\site-packages\\hsfs\\core\\arrow_flight_client.py\", line 427, in read_query\n",
      "    return self._get_dataset(\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\HP\\anaconda3\\Lib\\site-packages\\retrying.py\", line 56, in wrapped_f\n",
      "    return Retrying(*dargs, **dkw).call(f, *args, **kw)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\HP\\anaconda3\\Lib\\site-packages\\retrying.py\", line 257, in call\n",
      "    return attempt.get(self._wrap_exception)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\HP\\anaconda3\\Lib\\site-packages\\retrying.py\", line 301, in get\n",
      "    six.reraise(self.value[0], self.value[1], self.value[2])\n",
      "  File \"C:\\Users\\HP\\anaconda3\\Lib\\site-packages\\six.py\", line 719, in reraise\n",
      "    raise value\n",
      "  File \"C:\\Users\\HP\\anaconda3\\Lib\\site-packages\\retrying.py\", line 251, in call\n",
      "    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)\n",
      "                      ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\HP\\anaconda3\\Lib\\site-packages\\hsfs\\core\\arrow_flight_client.py\", line 413, in _get_dataset\n",
      "    reader = self._connection.do_get(info.endpoints[0].ticket, options)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"pyarrow\\\\_flight.pyx\", line 1636, in pyarrow._flight.FlightClient.do_get\n",
      "  File \"pyarrow\\\\_flight.pyx\", line 58, in pyarrow._flight.check_flight_status\n",
      "pyarrow._flight.FlightServerError: [Errno 2] Opening HDFS file '/apps/hive/warehouse/mlops101_featurestore.db/aqi_predictions_1/.hoodie/hoodie.properties' failed. Detail: [errno 2] No such file or directory. Detail: Python exception: FlyingDuckException. gRPC client debug context: UNKNOWN:Error received from peer ipv4:51.79.26.27:5005 {grpc_message:\"[Errno 2] Opening HDFS file \\'/apps/hive/warehouse/mlops101_featurestore.db/aqi_predictions_1/.hoodie/hoodie.properties\\' failed. Detail: [errno 2] No such file or directory. Detail: Python exception: FlyingDuckException\", grpc_status:2, created_time:\"2025-01-03T00:48:06.541484656+00:00\"}. Client context: IOError: Server never sent a data message. Detail: Internal\n",
      "Error: Reading data from Hopsworks, using Hopsworks Feature Query Service           \n"
     ]
    },
    {
     "ename": "FeatureStoreException",
     "evalue": "Could not read data using Hopsworks Feature Query Service.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFlightServerError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\hsfs\\core\\arrow_flight_client.py:364\u001b[0m, in \u001b[0;36mArrowFlightClient._handle_afs_exception.<locals>.decorator.<locals>.afs_error_handler_wrapper\u001b[1;34m(instance, *args, **kw)\u001b[0m\n\u001b[0;32m    363\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 364\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(instance, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    365\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\hsfs\\core\\arrow_flight_client.py:427\u001b[0m, in \u001b[0;36mArrowFlightClient.read_query\u001b[1;34m(self, query_object, arrow_flight_config, dataframe_type)\u001b[0m\n\u001b[0;32m    426\u001b[0m descriptor \u001b[38;5;241m=\u001b[39m pyarrow\u001b[38;5;241m.\u001b[39mflight\u001b[38;5;241m.\u001b[39mFlightDescriptor\u001b[38;5;241m.\u001b[39mfor_command(query_encoded)\n\u001b[1;32m--> 427\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_dataset(\n\u001b[0;32m    428\u001b[0m     descriptor,\n\u001b[0;32m    429\u001b[0m     (\n\u001b[0;32m    430\u001b[0m         arrow_flight_config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout)\n\u001b[0;32m    431\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m arrow_flight_config\n\u001b[0;32m    432\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout\n\u001b[0;32m    433\u001b[0m     ),\n\u001b[0;32m    434\u001b[0m     dataframe_type,\n\u001b[0;32m    435\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\retrying.py:56\u001b[0m, in \u001b[0;36mretry.<locals>.wrap.<locals>.wrapped_f\u001b[1;34m(*args, **kw)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;129m@six\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(f)\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped_f\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw):\n\u001b[1;32m---> 56\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Retrying(\u001b[38;5;241m*\u001b[39mdargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdkw)\u001b[38;5;241m.\u001b[39mcall(f, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\retrying.py:257\u001b[0m, in \u001b[0;36mRetrying.call\u001b[1;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m    256\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshould_reject(attempt):\n\u001b[1;32m--> 257\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m attempt\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wrap_exception)\n\u001b[0;32m    259\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_after_attempts:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\retrying.py:301\u001b[0m, in \u001b[0;36mAttempt.get\u001b[1;34m(self, wrap_exception)\u001b[0m\n\u001b[0;32m    300\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 301\u001b[0m         six\u001b[38;5;241m.\u001b[39mreraise(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalue[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalue[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalue[\u001b[38;5;241m2\u001b[39m])\n\u001b[0;32m    302\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\six.py:719\u001b[0m, in \u001b[0;36mreraise\u001b[1;34m(tp, value, tb)\u001b[0m\n\u001b[0;32m    718\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m value\u001b[38;5;241m.\u001b[39mwith_traceback(tb)\n\u001b[1;32m--> 719\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m value\n\u001b[0;32m    720\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\retrying.py:251\u001b[0m, in \u001b[0;36mRetrying.call\u001b[1;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 251\u001b[0m     attempt \u001b[38;5;241m=\u001b[39m Attempt(fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs), attempt_number, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    252\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\hsfs\\core\\arrow_flight_client.py:413\u001b[0m, in \u001b[0;36mArrowFlightClient._get_dataset\u001b[1;34m(self, descriptor, timeout, dataframe_type)\u001b[0m\n\u001b[0;32m    412\u001b[0m options \u001b[38;5;241m=\u001b[39m pyarrow\u001b[38;5;241m.\u001b[39mflight\u001b[38;5;241m.\u001b[39mFlightCallOptions(timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m--> 413\u001b[0m reader \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connection\u001b[38;5;241m.\u001b[39mdo_get(info\u001b[38;5;241m.\u001b[39mendpoints[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mticket, options)\n\u001b[0;32m    414\u001b[0m _logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset fetched. Converting to dataframe \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, dataframe_type)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pyarrow\\_flight.pyx:1636\u001b[0m, in \u001b[0;36mpyarrow._flight.FlightClient.do_get\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pyarrow\\_flight.pyx:58\u001b[0m, in \u001b[0;36mpyarrow._flight.check_flight_status\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFlightServerError\u001b[0m: [Errno 2] Opening HDFS file '/apps/hive/warehouse/mlops101_featurestore.db/aqi_predictions_1/.hoodie/hoodie.properties' failed. Detail: [errno 2] No such file or directory. Detail: Python exception: FlyingDuckException. gRPC client debug context: UNKNOWN:Error received from peer ipv4:51.79.26.27:5005 {grpc_message:\"[Errno 2] Opening HDFS file \\'/apps/hive/warehouse/mlops101_featurestore.db/aqi_predictions_1/.hoodie/hoodie.properties\\' failed. Detail: [errno 2] No such file or directory. Detail: Python exception: FlyingDuckException\", grpc_status:2, created_time:\"2025-01-03T00:48:06.541484656+00:00\"}. Client context: IOError: Server never sent a data message. Detail: Internal",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mFeatureStoreException\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m history_df \u001b[38;5;241m=\u001b[39m monitor_fg\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m      2\u001b[0m history_df\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\hsfs\\feature_group.py:2509\u001b[0m, in \u001b[0;36mFeatureGroup.read\u001b[1;34m(self, wallclock_time, online, dataframe_type, read_options)\u001b[0m\n\u001b[0;32m   2499\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m   2500\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mselect_all()\n\u001b[0;32m   2501\u001b[0m         \u001b[38;5;241m.\u001b[39mas_of(wallclock_time)\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2506\u001b[0m         )\n\u001b[0;32m   2507\u001b[0m     )\n\u001b[0;32m   2508\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mselect_all()\u001b[38;5;241m.\u001b[39mread(\n\u001b[0;32m   2510\u001b[0m         online,\n\u001b[0;32m   2511\u001b[0m         dataframe_type,\n\u001b[0;32m   2512\u001b[0m         read_options \u001b[38;5;129;01mor\u001b[39;00m {},\n\u001b[0;32m   2513\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\hsfs\\constructor\\query.py:206\u001b[0m, in \u001b[0;36mQuery.read\u001b[1;34m(self, online, dataframe_type, read_options)\u001b[0m\n\u001b[0;32m    201\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjoins) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m [f\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m schema]:\n\u001b[0;32m    202\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    203\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPandas types casting only supported for feature_group.read()/query.select_all()\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    204\u001b[0m         )\n\u001b[1;32m--> 206\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m engine\u001b[38;5;241m.\u001b[39mget_instance()\u001b[38;5;241m.\u001b[39msql(\n\u001b[0;32m    207\u001b[0m     sql_query,\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_feature_store_name,\n\u001b[0;32m    209\u001b[0m     online_conn,\n\u001b[0;32m    210\u001b[0m     dataframe_type,\n\u001b[0;32m    211\u001b[0m     read_options,\n\u001b[0;32m    212\u001b[0m     schema,\n\u001b[0;32m    213\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\hsfs\\engine\\python.py:146\u001b[0m, in \u001b[0;36mEngine.sql\u001b[1;34m(self, sql_query, feature_store, online_conn, dataframe_type, read_options, schema)\u001b[0m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msql\u001b[39m(\n\u001b[0;32m    137\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    138\u001b[0m     sql_query: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    143\u001b[0m     schema: Optional[List[feature\u001b[38;5;241m.\u001b[39mFeature]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    144\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[pd\u001b[38;5;241m.\u001b[39mDataFrame, pl\u001b[38;5;241m.\u001b[39mDataFrame]:\n\u001b[0;32m    145\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m online_conn:\n\u001b[1;32m--> 146\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sql_offline(\n\u001b[0;32m    147\u001b[0m             sql_query,\n\u001b[0;32m    148\u001b[0m             dataframe_type,\n\u001b[0;32m    149\u001b[0m             schema,\n\u001b[0;32m    150\u001b[0m             arrow_flight_config\u001b[38;5;241m=\u001b[39mread_options\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marrow_flight_config\u001b[39m\u001b[38;5;124m\"\u001b[39m, {})\n\u001b[0;32m    151\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m read_options\n\u001b[0;32m    152\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m {},\n\u001b[0;32m    153\u001b[0m         )\n\u001b[0;32m    154\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    155\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jdbc(\n\u001b[0;32m    156\u001b[0m             sql_query, online_conn, dataframe_type, read_options, schema\n\u001b[0;32m    157\u001b[0m         )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\hsfs\\engine\\python.py:189\u001b[0m, in \u001b[0;36mEngine._sql_offline\u001b[1;34m(self, sql_query, dataframe_type, schema, arrow_flight_config)\u001b[0m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(sql_query, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery_string\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m sql_query:\n\u001b[0;32m    187\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhsfs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m arrow_flight_client\n\u001b[1;32m--> 189\u001b[0m     result_df \u001b[38;5;241m=\u001b[39m util\u001b[38;5;241m.\u001b[39mrun_with_loading_animation(\n\u001b[0;32m    190\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReading data from Hopsworks, using Hopsworks Feature Query Service\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    191\u001b[0m         arrow_flight_client\u001b[38;5;241m.\u001b[39mget_instance()\u001b[38;5;241m.\u001b[39mread_query,\n\u001b[0;32m    192\u001b[0m         sql_query,\n\u001b[0;32m    193\u001b[0m         arrow_flight_config \u001b[38;5;129;01mor\u001b[39;00m {},\n\u001b[0;32m    194\u001b[0m         dataframe_type,\n\u001b[0;32m    195\u001b[0m     )\n\u001b[0;32m    196\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    197\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    198\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReading data with Hive is not supported when using hopsworks client version >= 4.0\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    199\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\hopsworks_common\\util.py:303\u001b[0m, in \u001b[0;36mrun_with_loading_animation\u001b[1;34m(message, func, *args, **kwargs)\u001b[0m\n\u001b[0;32m    300\u001b[0m end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    302\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 303\u001b[0m     result \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    304\u001b[0m     end \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m    305\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\hsfs\\core\\arrow_flight_client.py:382\u001b[0m, in \u001b[0;36mArrowFlightClient._handle_afs_exception.<locals>.decorator.<locals>.afs_error_handler_wrapper\u001b[1;34m(instance, *args, **kw)\u001b[0m\n\u001b[0;32m    380\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m FeatureStoreException(\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDetails:\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    381\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 382\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m FeatureStoreException(user_message) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[1;31mFeatureStoreException\u001b[0m: Could not read data using Hopsworks Feature Query Service."
     ]
    }
   ],
   "source": [
    "history_df = monitor_fg.read()\n",
    "history_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
