{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d2kLrOh-bpGy"
   },
   "source": [
    "# Iris Flower - Batch Prediction\n",
    "\n",
    "\n",
    "In this notebook we will, \n",
    "\n",
    "1. Load the batch inference data that arrived in the last 24 hours\n",
    "2. Predict the first Iris Flower found in the batch\n",
    "3. Write the ouput png of the Iris flower predicted, to be displayed in Github Pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "xRtpj-psbpG8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-30 20:43:12,388 INFO: Initializing external client\n",
      "2024-12-30 20:43:12,390 INFO: Base URL: https://c.app.hopsworks.ai:443\n",
      "2024-12-30 20:43:18,812 INFO: Python Engine initialized.\n",
      "\n",
      "Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/1207459\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import hopsworks\n",
    "import joblib\n",
    "\n",
    "project = hopsworks.login()\n",
    "\n",
    "fs = project.get_feature_store()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading model artifact (0 dirs, 2 files)... DONE\r"
     ]
    }
   ],
   "source": [
    "mr = project.get_model_registry()\n",
    "model = mr.get_model(\"iris\", version=1)\n",
    "model_dir = model.download()\n",
    "model = joblib.load(model_dir + \"/iris_model.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are downloading the 'raw' iris data. We explicitly do not want transformed data, reading for training. \n",
    "\n",
    "So, let's download the iris dataset, and preview some rows. \n",
    "\n",
    "Note, that it is 'tabular data'. There are 5 columns: 4 of them are \"features\", and the \"variety\" column is the **target** (what we are trying to predict using the 4 feature values in the target's row)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "nRmFM7vcbpHA",
    "outputId": "d920d168-9818-40c5-c292-4cf0afcbbcfd"
   },
   "outputs": [],
   "source": [
    "feature_view = fs.get_feature_view(name=\"iris\", version=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will do some **Batch Inference**. \n",
    "\n",
    "We will read all the input features that have arrived in the last 24 hours, and score them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "uHuAD3ttP8Ep"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished: Reading data from Hopsworks, using Hopsworks Feature Query Service (0.96s) \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['Setosa', 'Versicolor', 'Setosa', 'Setosa', 'Setosa', 'Versicolor',\n",
       "       'Virginica', 'Setosa', 'Virginica', 'Setosa', 'Setosa', 'Setosa',\n",
       "       'Versicolor', 'Setosa', 'Versicolor', 'Virginica', 'Setosa',\n",
       "       'Virginica', 'Virginica', 'Virginica', 'Versicolor', 'Virginica',\n",
       "       'Setosa', 'Setosa', 'Setosa', 'Versicolor', 'Virginica',\n",
       "       'Versicolor', 'Versicolor', 'Virginica', 'Virginica', 'Setosa',\n",
       "       'Setosa', 'Setosa', 'Versicolor', 'Setosa', 'Versicolor',\n",
       "       'Versicolor', 'Virginica', 'Versicolor', 'Versicolor',\n",
       "       'Versicolor', 'Versicolor', 'Virginica', 'Versicolor', 'Setosa',\n",
       "       'Virginica', 'Virginica', 'Setosa', 'Setosa', 'Versicolor',\n",
       "       'Versicolor', 'Virginica', 'Setosa', 'Virginica', 'Setosa',\n",
       "       'Versicolor', 'Versicolor', 'Virginica', 'Setosa', 'Virginica',\n",
       "       'Setosa', 'Setosa', 'Virginica', 'Setosa', 'Versicolor', 'Setosa',\n",
       "       'Virginica', 'Versicolor', 'Setosa', 'Virginica', 'Versicolor',\n",
       "       'Virginica', 'Virginica', 'Virginica', 'Versicolor', 'Versicolor',\n",
       "       'Versicolor', 'Virginica', 'Virginica', 'Setosa', 'Setosa',\n",
       "       'Virginica', 'Virginica', 'Virginica', 'Virginica', 'Versicolor',\n",
       "       'Setosa', 'Setosa', 'Setosa', 'Setosa', 'Versicolor', 'Setosa',\n",
       "       'Versicolor', 'Virginica', 'Virginica', 'Versicolor', 'Versicolor',\n",
       "       'Setosa', 'Versicolor', 'Versicolor', 'Virginica', 'Virginica',\n",
       "       'Versicolor', 'Versicolor', 'Versicolor', 'Versicolor',\n",
       "       'Versicolor', 'Setosa', 'Virginica', 'Setosa', 'Virginica',\n",
       "       'Versicolor', 'Setosa', 'Versicolor', 'Virginica', 'Versicolor',\n",
       "       'Versicolor', 'Virginica', 'Versicolor', 'Virginica', 'Virginica',\n",
       "       'Setosa', 'Virginica', 'Setosa', 'Setosa', 'Versicolor',\n",
       "       'Virginica', 'Versicolor', 'Setosa', 'Versicolor', 'Versicolor',\n",
       "       'Versicolor', 'Virginica', 'Versicolor', 'Versicolor', 'Setosa',\n",
       "       'Setosa', 'Setosa', 'Setosa', 'Versicolor', 'Setosa', 'Setosa',\n",
       "       'Setosa', 'Versicolor', 'Virginica', 'Virginica', 'Versicolor',\n",
       "       'Versicolor', 'Versicolor', 'Versicolor', 'Versicolor', 'Setosa',\n",
       "       'Setosa', 'Setosa', 'Setosa', 'Setosa', 'Setosa', 'Setosa',\n",
       "       'Setosa', 'Setosa', 'Setosa', 'Setosa', 'Setosa', 'Setosa',\n",
       "       'Setosa', 'Setosa', 'Setosa', 'Setosa', 'Setosa', 'Setosa',\n",
       "       'Setosa', 'Setosa', 'Setosa', 'Setosa', 'Setosa', 'Setosa',\n",
       "       'Setosa', 'Setosa', 'Setosa', 'Versicolor', 'Versicolor', 'Setosa',\n",
       "       'Virginica', 'Versicolor'], dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime\n",
    "from PIL import Image\n",
    "\n",
    "#the batch data is the feature data, i.e without the labels\n",
    "batch_data = feature_view.get_batch_data()\n",
    "\n",
    "y_pred = model.predict(batch_data)\n",
    "\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      5.000000\n",
       "1      6.600000\n",
       "2      5.100000\n",
       "3      5.100000\n",
       "4      5.400000\n",
       "         ...   \n",
       "180    6.533010\n",
       "181    4.995918\n",
       "182    5.199865\n",
       "183    5.946969\n",
       "184    5.596305\n",
       "Name: sepal_length, Length: 185, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_data['sepal_length']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch prediction output is the last entry in the batch - it is output as a file 'latest_iris.png'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#image of the predicted flower, the latest data added to the feature store\n",
    "flower = y_pred[y_pred.size-1]\n",
    "flower_img = \"assets/\" + flower + \".png\"\n",
    "img = Image.open(flower_img)            \n",
    "\n",
    "img.save(\"../../assets/latest_iris.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished: Reading data from Hopsworks, using Hopsworks Feature Query Service (1.31s) \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>variety</th>\n",
       "      <th>uuid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>3.300000</td>\n",
       "      <td>1.400000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>Setosa</td>\n",
       "      <td>48fe1330-1f4f-4526-8bb6-2f3592ab7b61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.600000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.400000</td>\n",
       "      <td>1.400000</td>\n",
       "      <td>Versicolor</td>\n",
       "      <td>9b23506a-83b3-47fd-ac8f-8acfa716837d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.100000</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>1.400000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>Setosa</td>\n",
       "      <td>18383b75-a81b-4527-9cde-59b047a66bca</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.100000</td>\n",
       "      <td>3.700000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>Setosa</td>\n",
       "      <td>0dd9d1fb-1d52-4df5-a6f2-b2c1a49c5db0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.400000</td>\n",
       "      <td>3.400000</td>\n",
       "      <td>1.700000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>Setosa</td>\n",
       "      <td>eb0183b4-f3a2-4248-9b8b-a661f1bbcd54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>6.533010</td>\n",
       "      <td>3.296076</td>\n",
       "      <td>3.580095</td>\n",
       "      <td>1.754300</td>\n",
       "      <td>Versicolor</td>\n",
       "      <td>4ce96f90-c87f-476d-8303-b0ea8890f9fe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>4.995918</td>\n",
       "      <td>2.899197</td>\n",
       "      <td>4.410009</td>\n",
       "      <td>1.447099</td>\n",
       "      <td>Versicolor</td>\n",
       "      <td>3c5d6941-fef4-4ace-b8a6-dcb885b74ab3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>5.199865</td>\n",
       "      <td>2.674191</td>\n",
       "      <td>1.915063</td>\n",
       "      <td>0.554497</td>\n",
       "      <td>Setosa</td>\n",
       "      <td>7959ca6c-db6a-4ff7-8710-d310807c155f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>5.946969</td>\n",
       "      <td>2.400497</td>\n",
       "      <td>5.845257</td>\n",
       "      <td>2.328123</td>\n",
       "      <td>Virginica</td>\n",
       "      <td>37183735-3782-48d0-b530-452106783ce2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>5.596305</td>\n",
       "      <td>2.714259</td>\n",
       "      <td>4.625981</td>\n",
       "      <td>1.428525</td>\n",
       "      <td>Virginica</td>\n",
       "      <td>b28c3f8b-8e74-4446-8cee-eb3002906035</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>185 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     sepal_length  sepal_width  petal_length  petal_width     variety  \\\n",
       "0        5.000000     3.300000      1.400000     0.200000      Setosa   \n",
       "1        6.600000     3.000000      4.400000     1.400000  Versicolor   \n",
       "2        5.100000     3.500000      1.400000     0.300000      Setosa   \n",
       "3        5.100000     3.700000      1.500000     0.400000      Setosa   \n",
       "4        5.400000     3.400000      1.700000     0.200000      Setosa   \n",
       "..            ...          ...           ...          ...         ...   \n",
       "180      6.533010     3.296076      3.580095     1.754300  Versicolor   \n",
       "181      4.995918     2.899197      4.410009     1.447099  Versicolor   \n",
       "182      5.199865     2.674191      1.915063     0.554497      Setosa   \n",
       "183      5.946969     2.400497      5.845257     2.328123   Virginica   \n",
       "184      5.596305     2.714259      4.625981     1.428525   Virginica   \n",
       "\n",
       "                                     uuid  \n",
       "0    48fe1330-1f4f-4526-8bb6-2f3592ab7b61  \n",
       "1    9b23506a-83b3-47fd-ac8f-8acfa716837d  \n",
       "2    18383b75-a81b-4527-9cde-59b047a66bca  \n",
       "3    0dd9d1fb-1d52-4df5-a6f2-b2c1a49c5db0  \n",
       "4    eb0183b4-f3a2-4248-9b8b-a661f1bbcd54  \n",
       "..                                    ...  \n",
       "180  4ce96f90-c87f-476d-8303-b0ea8890f9fe  \n",
       "181  3c5d6941-fef4-4ace-b8a6-dcb885b74ab3  \n",
       "182  7959ca6c-db6a-4ff7-8710-d310807c155f  \n",
       "183  37183735-3782-48d0-b530-452106783ce2  \n",
       "184  b28c3f8b-8e74-4446-8cee-eb3002906035  \n",
       "\n",
       "[185 rows x 6 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris_fg = fs.get_feature_group(name=\"iris\", version=1)\n",
    "df = iris_fg.read()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Virginica'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label = df.iloc[-1][\"variety\"]\n",
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#image of the actual flower\n",
    "label_flower = \"assets/\" + label + \".png\"\n",
    "\n",
    "img = Image.open(label_flower)            \n",
    "\n",
    "img.save(\"../../assets/actual_iris.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "monitor_fg = fs.get_or_create_feature_group(name=\"iris_predictions\",\n",
    "                                  version=1,\n",
    "                                  primary_key=[\"datetime\"],\n",
    "                                  description=\"Iris flower Prediction/Outcome Monitoring\"\n",
    "                                 )\n",
    "\n",
    "# Clear the contents of the feature group\n",
    "# monitor_fg = fs.get_feature_group(name=\"iris_predictions\", version=1)\n",
    "\n",
    "# monitor_fg.delete()\n",
    "\n",
    "# print(\"Feature group contents cleared successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Group created successfully, explore it at \n",
      "https://c.app.hopsworks.ai:443/p/1207459/fs/1195092/fg/1393393\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading Dataframe: 100.00% |██████████| Rows 1/1 | Elapsed Time: 00:02 | Remaining Time: 00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching job: iris_predictions_1_offline_fg_materialization\n",
      "Job started successfully, you can follow the progress at \n",
      "https://c.app.hopsworks.ai:443/p/1207459/jobs/named/iris_predictions_1_offline_fg_materialization/executions\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Job('iris_predictions_1_offline_fg_materialization', 'SPARK'), None)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "now = datetime.now().strftime(\"%m/%d/%Y, %H:%M:%S\")\n",
    "\n",
    "data = {\n",
    "    's_length': [float(df.iloc[-1]['sepal_length'])],  # Convert to scalar\n",
    "    's_width': [float(df.iloc[-1]['sepal_width'])],\n",
    "    'p_length': [float(df.iloc[-1]['petal_length'])],\n",
    "    'p_width': [float(df.iloc[-1]['petal_width'])],\n",
    "    'prediction': [flower],  # Ensure 'flower' is a scalar value\n",
    "    'label': [label],        # Ensure 'label' is a scalar value\n",
    "    'datetime': [now],\n",
    "}\n",
    "monitor_df = pd.DataFrame(data)\n",
    "monitor_fg.insert(monitor_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-30 20:37:23,541 ERROR: [Errno 2] Opening HDFS file '/apps/hive/warehouse/mlops101_featurestore.db/iris_predictions_1/.hoodie/hoodie.properties' failed. Detail: [errno 2] No such file or directory. Detail: Python exception: FlyingDuckException. gRPC client debug context: UNKNOWN:Error received from peer ipv4:51.79.26.27:5005 {created_time:\"2024-12-30T19:37:23.540061185+00:00\", grpc_status:2, grpc_message:\"[Errno 2] Opening HDFS file \\'/apps/hive/warehouse/mlops101_featurestore.db/iris_predictions_1/.hoodie/hoodie.properties\\' failed. Detail: [errno 2] No such file or directory. Detail: Python exception: FlyingDuckException\"}. Client context: IOError: Server never sent a data message. Detail: Internal\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\HP\\anaconda3\\Lib\\site-packages\\hsfs\\core\\arrow_flight_client.py\", line 364, in afs_error_handler_wrapper\n",
      "    return func(instance, *args, **kw)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\HP\\anaconda3\\Lib\\site-packages\\hsfs\\core\\arrow_flight_client.py\", line 427, in read_query\n",
      "    return self._get_dataset(\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\HP\\anaconda3\\Lib\\site-packages\\retrying.py\", line 56, in wrapped_f\n",
      "    return Retrying(*dargs, **dkw).call(f, *args, **kw)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\HP\\anaconda3\\Lib\\site-packages\\retrying.py\", line 257, in call\n",
      "    return attempt.get(self._wrap_exception)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\HP\\anaconda3\\Lib\\site-packages\\retrying.py\", line 301, in get\n",
      "    six.reraise(self.value[0], self.value[1], self.value[2])\n",
      "  File \"C:\\Users\\HP\\anaconda3\\Lib\\site-packages\\six.py\", line 719, in reraise\n",
      "    raise value\n",
      "  File \"C:\\Users\\HP\\anaconda3\\Lib\\site-packages\\retrying.py\", line 251, in call\n",
      "    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)\n",
      "                      ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\HP\\anaconda3\\Lib\\site-packages\\hsfs\\core\\arrow_flight_client.py\", line 413, in _get_dataset\n",
      "    reader = self._connection.do_get(info.endpoints[0].ticket, options)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"pyarrow\\\\_flight.pyx\", line 1636, in pyarrow._flight.FlightClient.do_get\n",
      "  File \"pyarrow\\\\_flight.pyx\", line 58, in pyarrow._flight.check_flight_status\n",
      "pyarrow._flight.FlightServerError: [Errno 2] Opening HDFS file '/apps/hive/warehouse/mlops101_featurestore.db/iris_predictions_1/.hoodie/hoodie.properties' failed. Detail: [errno 2] No such file or directory. Detail: Python exception: FlyingDuckException. gRPC client debug context: UNKNOWN:Error received from peer ipv4:51.79.26.27:5005 {created_time:\"2024-12-30T19:37:23.540061185+00:00\", grpc_status:2, grpc_message:\"[Errno 2] Opening HDFS file \\'/apps/hive/warehouse/mlops101_featurestore.db/iris_predictions_1/.hoodie/hoodie.properties\\' failed. Detail: [errno 2] No such file or directory. Detail: Python exception: FlyingDuckException\"}. Client context: IOError: Server never sent a data message. Detail: Internal\n",
      "Error: Reading data from Hopsworks, using Hopsworks Feature Query Service           \n"
     ]
    },
    {
     "ename": "FeatureStoreException",
     "evalue": "Could not read data using Hopsworks Feature Query Service.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFlightServerError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\hsfs\\core\\arrow_flight_client.py:364\u001b[0m, in \u001b[0;36mArrowFlightClient._handle_afs_exception.<locals>.decorator.<locals>.afs_error_handler_wrapper\u001b[1;34m(instance, *args, **kw)\u001b[0m\n\u001b[0;32m    363\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 364\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(instance, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    365\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\hsfs\\core\\arrow_flight_client.py:427\u001b[0m, in \u001b[0;36mArrowFlightClient.read_query\u001b[1;34m(self, query_object, arrow_flight_config, dataframe_type)\u001b[0m\n\u001b[0;32m    426\u001b[0m descriptor \u001b[38;5;241m=\u001b[39m pyarrow\u001b[38;5;241m.\u001b[39mflight\u001b[38;5;241m.\u001b[39mFlightDescriptor\u001b[38;5;241m.\u001b[39mfor_command(query_encoded)\n\u001b[1;32m--> 427\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_dataset(\n\u001b[0;32m    428\u001b[0m     descriptor,\n\u001b[0;32m    429\u001b[0m     (\n\u001b[0;32m    430\u001b[0m         arrow_flight_config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout)\n\u001b[0;32m    431\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m arrow_flight_config\n\u001b[0;32m    432\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout\n\u001b[0;32m    433\u001b[0m     ),\n\u001b[0;32m    434\u001b[0m     dataframe_type,\n\u001b[0;32m    435\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\retrying.py:56\u001b[0m, in \u001b[0;36mretry.<locals>.wrap.<locals>.wrapped_f\u001b[1;34m(*args, **kw)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;129m@six\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(f)\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped_f\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw):\n\u001b[1;32m---> 56\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Retrying(\u001b[38;5;241m*\u001b[39mdargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdkw)\u001b[38;5;241m.\u001b[39mcall(f, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\retrying.py:257\u001b[0m, in \u001b[0;36mRetrying.call\u001b[1;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m    256\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshould_reject(attempt):\n\u001b[1;32m--> 257\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m attempt\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wrap_exception)\n\u001b[0;32m    259\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_after_attempts:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\retrying.py:301\u001b[0m, in \u001b[0;36mAttempt.get\u001b[1;34m(self, wrap_exception)\u001b[0m\n\u001b[0;32m    300\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 301\u001b[0m         six\u001b[38;5;241m.\u001b[39mreraise(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalue[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalue[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalue[\u001b[38;5;241m2\u001b[39m])\n\u001b[0;32m    302\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\six.py:719\u001b[0m, in \u001b[0;36mreraise\u001b[1;34m(tp, value, tb)\u001b[0m\n\u001b[0;32m    718\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m value\u001b[38;5;241m.\u001b[39mwith_traceback(tb)\n\u001b[1;32m--> 719\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m value\n\u001b[0;32m    720\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\retrying.py:251\u001b[0m, in \u001b[0;36mRetrying.call\u001b[1;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 251\u001b[0m     attempt \u001b[38;5;241m=\u001b[39m Attempt(fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs), attempt_number, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    252\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\hsfs\\core\\arrow_flight_client.py:413\u001b[0m, in \u001b[0;36mArrowFlightClient._get_dataset\u001b[1;34m(self, descriptor, timeout, dataframe_type)\u001b[0m\n\u001b[0;32m    412\u001b[0m options \u001b[38;5;241m=\u001b[39m pyarrow\u001b[38;5;241m.\u001b[39mflight\u001b[38;5;241m.\u001b[39mFlightCallOptions(timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m--> 413\u001b[0m reader \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connection\u001b[38;5;241m.\u001b[39mdo_get(info\u001b[38;5;241m.\u001b[39mendpoints[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mticket, options)\n\u001b[0;32m    414\u001b[0m _logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset fetched. Converting to dataframe \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, dataframe_type)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pyarrow\\_flight.pyx:1636\u001b[0m, in \u001b[0;36mpyarrow._flight.FlightClient.do_get\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pyarrow\\_flight.pyx:58\u001b[0m, in \u001b[0;36mpyarrow._flight.check_flight_status\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFlightServerError\u001b[0m: [Errno 2] Opening HDFS file '/apps/hive/warehouse/mlops101_featurestore.db/iris_predictions_1/.hoodie/hoodie.properties' failed. Detail: [errno 2] No such file or directory. Detail: Python exception: FlyingDuckException. gRPC client debug context: UNKNOWN:Error received from peer ipv4:51.79.26.27:5005 {created_time:\"2024-12-30T19:37:23.540061185+00:00\", grpc_status:2, grpc_message:\"[Errno 2] Opening HDFS file \\'/apps/hive/warehouse/mlops101_featurestore.db/iris_predictions_1/.hoodie/hoodie.properties\\' failed. Detail: [errno 2] No such file or directory. Detail: Python exception: FlyingDuckException\"}. Client context: IOError: Server never sent a data message. Detail: Internal",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mFeatureStoreException\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m history_df \u001b[38;5;241m=\u001b[39m monitor_fg\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m      2\u001b[0m history_df\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\hsfs\\feature_group.py:2509\u001b[0m, in \u001b[0;36mFeatureGroup.read\u001b[1;34m(self, wallclock_time, online, dataframe_type, read_options)\u001b[0m\n\u001b[0;32m   2499\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m   2500\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mselect_all()\n\u001b[0;32m   2501\u001b[0m         \u001b[38;5;241m.\u001b[39mas_of(wallclock_time)\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2506\u001b[0m         )\n\u001b[0;32m   2507\u001b[0m     )\n\u001b[0;32m   2508\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mselect_all()\u001b[38;5;241m.\u001b[39mread(\n\u001b[0;32m   2510\u001b[0m         online,\n\u001b[0;32m   2511\u001b[0m         dataframe_type,\n\u001b[0;32m   2512\u001b[0m         read_options \u001b[38;5;129;01mor\u001b[39;00m {},\n\u001b[0;32m   2513\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\hsfs\\constructor\\query.py:206\u001b[0m, in \u001b[0;36mQuery.read\u001b[1;34m(self, online, dataframe_type, read_options)\u001b[0m\n\u001b[0;32m    201\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjoins) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m [f\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m schema]:\n\u001b[0;32m    202\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    203\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPandas types casting only supported for feature_group.read()/query.select_all()\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    204\u001b[0m         )\n\u001b[1;32m--> 206\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m engine\u001b[38;5;241m.\u001b[39mget_instance()\u001b[38;5;241m.\u001b[39msql(\n\u001b[0;32m    207\u001b[0m     sql_query,\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_feature_store_name,\n\u001b[0;32m    209\u001b[0m     online_conn,\n\u001b[0;32m    210\u001b[0m     dataframe_type,\n\u001b[0;32m    211\u001b[0m     read_options,\n\u001b[0;32m    212\u001b[0m     schema,\n\u001b[0;32m    213\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\hsfs\\engine\\python.py:146\u001b[0m, in \u001b[0;36mEngine.sql\u001b[1;34m(self, sql_query, feature_store, online_conn, dataframe_type, read_options, schema)\u001b[0m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msql\u001b[39m(\n\u001b[0;32m    137\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    138\u001b[0m     sql_query: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    143\u001b[0m     schema: Optional[List[feature\u001b[38;5;241m.\u001b[39mFeature]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    144\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[pd\u001b[38;5;241m.\u001b[39mDataFrame, pl\u001b[38;5;241m.\u001b[39mDataFrame]:\n\u001b[0;32m    145\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m online_conn:\n\u001b[1;32m--> 146\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sql_offline(\n\u001b[0;32m    147\u001b[0m             sql_query,\n\u001b[0;32m    148\u001b[0m             dataframe_type,\n\u001b[0;32m    149\u001b[0m             schema,\n\u001b[0;32m    150\u001b[0m             arrow_flight_config\u001b[38;5;241m=\u001b[39mread_options\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marrow_flight_config\u001b[39m\u001b[38;5;124m\"\u001b[39m, {})\n\u001b[0;32m    151\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m read_options\n\u001b[0;32m    152\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m {},\n\u001b[0;32m    153\u001b[0m         )\n\u001b[0;32m    154\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    155\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jdbc(\n\u001b[0;32m    156\u001b[0m             sql_query, online_conn, dataframe_type, read_options, schema\n\u001b[0;32m    157\u001b[0m         )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\hsfs\\engine\\python.py:189\u001b[0m, in \u001b[0;36mEngine._sql_offline\u001b[1;34m(self, sql_query, dataframe_type, schema, arrow_flight_config)\u001b[0m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(sql_query, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery_string\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m sql_query:\n\u001b[0;32m    187\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhsfs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m arrow_flight_client\n\u001b[1;32m--> 189\u001b[0m     result_df \u001b[38;5;241m=\u001b[39m util\u001b[38;5;241m.\u001b[39mrun_with_loading_animation(\n\u001b[0;32m    190\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReading data from Hopsworks, using Hopsworks Feature Query Service\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    191\u001b[0m         arrow_flight_client\u001b[38;5;241m.\u001b[39mget_instance()\u001b[38;5;241m.\u001b[39mread_query,\n\u001b[0;32m    192\u001b[0m         sql_query,\n\u001b[0;32m    193\u001b[0m         arrow_flight_config \u001b[38;5;129;01mor\u001b[39;00m {},\n\u001b[0;32m    194\u001b[0m         dataframe_type,\n\u001b[0;32m    195\u001b[0m     )\n\u001b[0;32m    196\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    197\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    198\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReading data with Hive is not supported when using hopsworks client version >= 4.0\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    199\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\hopsworks_common\\util.py:303\u001b[0m, in \u001b[0;36mrun_with_loading_animation\u001b[1;34m(message, func, *args, **kwargs)\u001b[0m\n\u001b[0;32m    300\u001b[0m end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    302\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 303\u001b[0m     result \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    304\u001b[0m     end \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m    305\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\hsfs\\core\\arrow_flight_client.py:382\u001b[0m, in \u001b[0;36mArrowFlightClient._handle_afs_exception.<locals>.decorator.<locals>.afs_error_handler_wrapper\u001b[1;34m(instance, *args, **kw)\u001b[0m\n\u001b[0;32m    380\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m FeatureStoreException(\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDetails:\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    381\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 382\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m FeatureStoreException(user_message) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[1;31mFeatureStoreException\u001b[0m: Could not read data using Hopsworks Feature Query Service."
     ]
    }
   ],
   "source": [
    "history_df = monitor_fg.read()\n",
    "history_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DeprecationWarning: Support for bleach <5 will be removed in a future version of nbconvert\n"
     ]
    }
   ],
   "source": [
    "# import dataframe_image as dfi\n",
    "\n",
    "# df_recent = history_df.tail(5)\n",
    " \n",
    "# # If you exclude this image, you may have the same iris_latest.png and iris_actual.png files\n",
    "# # If no files have changed, the GH-action 'git commit/push' stage fails, failing your GH action (last step)\n",
    "# # This image, however, is always new, ensuring git commit/push will succeed.\n",
    "# dfi.export(df_recent, '../../assets/df_recent.png', table_conversion = 'matplotlib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1]\n",
      " [0 0]]\n"
     ]
    }
   ],
   "source": [
    "# from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# predictions = history_df[['prediction']]\n",
    "# labels = history_df[['label']]\n",
    "\n",
    "# results = confusion_matrix(labels, predictions)\n",
    "# print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run the batch inference pipeline more times until you get 3 different iris flowers\n"
     ]
    }
   ],
   "source": [
    "# from matplotlib import pyplot\n",
    "# import seaborn as sns\n",
    "\n",
    "# # Only create the confusion matrix when our iris_predictions feature group has examples of all 3 iris flowers\n",
    "# if results.shape == (3,3):\n",
    "\n",
    "#     df_cm = pd.DataFrame(results, ['True Setosa', 'True Versicolor', 'True Virginica'],\n",
    "#                          ['Pred Setosa', 'Pred Versicolor', 'Pred Virginica'])\n",
    "\n",
    "#     cm = sns.heatmap(df_cm, annot=True)\n",
    "\n",
    "#     fig = cm.get_figure()\n",
    "#     fig.savefig(\"../../assets/confusion_matrix.png\") \n",
    "#     df_cm\n",
    "# else:\n",
    "#     print(\"Run the batch inference pipeline more times until you get 3 different iris flowers\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
